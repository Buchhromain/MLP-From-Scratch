
# üß† MLP From Scratch

Ce projet impl√©mente un **Perceptron Multi-Couches (MLP)** **sans utiliser de frameworks de deep learning** comme TensorFlow ou PyTorch. Tout a √©t√© cod√© from scratch pour permettre une compr√©hension compl√®te des m√©canismes internes.

---

## üìö Objectif

Cr√©er une API d'entra√Ænement et d'inf√©rence pour un r√©seau de neurones MLP, avec les contraintes suivantes :

- ‚ö†Ô∏è Aucun framework AI (PyTorch, TensorFlow, etc.)
- üîÅ Architecture MLP flexible
- üß† Multiples fonctions d'activation
- üìä Classification et r√©gression
- üßÆ Initialisation des poids param√©trable
- üèÉ Optimiseurs SGD : Momentum, RMSProp, Adam
- üõë Crit√®res d‚Äôarr√™t personnalisables
- üßΩ R√©gularisation L1, L2, Elastic Net
- üìâ Suivi de la perte
- ‚úÖ Matrice de confusion
- üîÄ (Optionnel) Ex√©cution parall√®le (multi-thread/GPU)
- üîå (Optionnel) API Web REST
- üîß Helpers et structure modulaire

---

## üõ†Ô∏è Fonctionnalit√©s impl√©ment√©es

| Fonctionnalit√©                    | Statut      |
|----------------------------------|-------------|
| Architecture MLP modulaire       | ‚úÖ           |
| Fonctions d'activation (ReLU, Sigmoid, Tanh, etc.) | ‚úÖ |
| Classification (MNIST)          | ‚úÖ           |
| R√©gression (Boston Housing)     | ‚úÖ           |
| Initialisation Xavier / He / Al√©atoire | ‚úÖ    |
| Optimiseurs : SGD, Momentum, RMSProp, Adam | ‚úÖ  |
| Crit√®res d'arr√™t (convergence, patience, epochs) | ‚úÖ |
| R√©gularisation : L1, L2, ElasticNet | ‚úÖ         |
| Matrice de confusion             | ‚úÖ           |
| Suivi de la perte et visualisation | ‚úÖ         |
| API Python bas-niveau            | ‚úÖ           |
| API Web REST                     | üöß (en cours)|
| Ex√©cution multithread            | üöß (optionnel) |

---

## üìÅ Structure du projet

```
mlp-from-scratch/
‚îÇ
‚îú‚îÄ‚îÄ src/                     # Code source principal
‚îÇ   ‚îú‚îÄ‚îÄ core/                # MLP, layers, loss, activation
‚îÇ   ‚îú‚îÄ‚îÄ optim/               # Impl√©mentation des optimiseurs
‚îÇ   ‚îú‚îÄ‚îÄ utils/               # Fonctions utilitaires
‚îÇ   ‚îî‚îÄ‚îÄ api/                 # Interface d'entra√Ænement et d'inf√©rence
‚îÇ
‚îú‚îÄ‚îÄ data/                    # Jeux de donn√©es CSV
‚îÇ
‚îú‚îÄ‚îÄ results/                 # Mod√®les entra√Æn√©s, courbes, matrices
‚îÇ
‚îú‚îÄ‚îÄ mnist-in-csv/            # Dataset MNIST en format CSV
‚îÇ
‚îú‚îÄ‚îÄ mlp.js                   # Fichier principal d'ex√©cution
‚îú‚îÄ‚îÄ loss_plot.html           # Visualisation de la courbe de perte
‚îî‚îÄ‚îÄ README.md                # Ce fichier
```

---

## üì¶ Installation

```bash
git clone https://github.com/Buchhromain/MLP-From-Scratch.git
cd MLP-From-Scratch
npm install
```

> ‚ö†Ô∏è Si ton projet est en JS (Node.js), adapte selon le runtime (ou `python` si tu es pass√© par un autre langage).

---

## üöÄ Lancer un entra√Ænement (exemple)

```bash
node mlp.js --mode train --dataset mnist-in-csv/mnist_train.csv
```

---

## üìä R√©sultats attendus

- ‚úÖ Sauvegarde du mod√®le entra√Æn√©
- ‚úÖ Matrice de confusion affich√©e
- ‚úÖ Courbe de perte g√©n√©r√©e (`loss_plot.html`)
- ‚úÖ √âvaluation sur test set
- ‚úÖ Fichier `model.json` sauvegard√©

---

## üìà Exemple de visualisation

*(√† ajouter plus tard : image ou capture de ta courbe de perte ou de la matrice de confusion)*

---

## üìÑ Sources de donn√©es

- [MNIST CSV](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv)
- [Boston Housing](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)

---

## üß† Con√ßu sans frameworks ML

Ce projet est enti√®rement fait maison pour garantir une **compr√©hension compl√®te du processus de deep learning**, de la propagation avant au backpropagation.

---

## üìå √Ä faire

- [ ] API Web REST (Express ou FastAPI)
- [ ] Support GPU avec WebGL ou CUDA (optionnel)
- [ ] Interface utilisateur simple

---

## üôã‚Äç‚ôÇÔ∏è Auteur

**Romain "Buchhromain"**  
> √âtudiant passionn√©, ce projet est une d√©monstration de compr√©hension et de savoir-faire algorithmique autour des r√©seaux de neurones.
