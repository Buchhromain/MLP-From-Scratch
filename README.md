
# ðŸ§  MLP From Scratch

This project implements a **Multi-Layer Perceptron (MLP)** **without using any deep learning frameworks** such as TensorFlow or PyTorch. Everything is built from the ground up to ensure a complete understanding of internal mechanisms.

---

## ðŸ“š Objective

Build a training and inference API for an MLP neural network, fulfilling the following requirements:

- âš ï¸ No AI frameworks (e.g., PyTorch, TensorFlow, etc.)
- ðŸ” Flexible MLP architecture
- ðŸ§  Multiple activation functions
- ðŸ“Š Support for classification and regression
- ðŸ§® Customizable weight initialization
- ðŸƒ SGD optimizers: Momentum, RMSProp, Adam
- ðŸ›‘ Customizable stopping criteria
- ðŸ§½ L1, L2, and Elastic Net regularization
- ðŸ“‰ Loss tracking and visualization
- âœ… Confusion matrix computation
- ðŸ”€ (Optional) Parallel training (multi-thread/GPU)
- ðŸ”Œ (Optional) Web API interface
- ðŸ”§ Helper classes and modular structure

---

## ðŸ› ï¸ Implemented Features

| Feature                          | Status      |
|----------------------------------|-------------|
| Modular MLP architecture         | âœ…           |
| Activation functions (ReLU, Sigmoid, Tanh, etc.) | âœ… |
| Classification (MNIST)          | âœ…           |
| Regression (Boston Housing)     | âœ…           |
| Xavier / He / Random initialization | âœ…        |
| Optimizers: SGD, Momentum, RMSProp, Adam | âœ…     |
| Stopping criteria (convergence, patience, epochs) | âœ… |
| Regularization: L1, L2, ElasticNet | âœ…         |
| Confusion matrix                 | âœ…           |
| Loss tracking and visualization | âœ…           |
| Low-level API implementation    | âœ…           |
| REST API Web Interface          | ðŸš§ (in progress) |
| Multithread support             | ðŸš§ (optional) |

---

## ðŸ“ Project Structure

```
mlp-from-scratch/
â”‚
â”œâ”€â”€ src/                     # Core source code
â”‚   â”œâ”€â”€ core/                # MLP, layers, loss, activations
â”‚   â”œâ”€â”€ optim/               # Optimizer implementations
â”‚   â”œâ”€â”€ utils/               # Utility functions
â”‚   â””â”€â”€ api/                 # Training/inference interface
â”‚
â”œâ”€â”€ data/                    # CSV datasets
â”‚
â”œâ”€â”€ results/                 # Trained models, plots, confusion matrices
â”‚
â”œâ”€â”€ mnist-in-csv/            # MNIST dataset in CSV format
â”‚
â”œâ”€â”€ mlp.js                   # Main entry point
â”œâ”€â”€ loss_plot.html           # Loss curve visualization
â””â”€â”€ README.md                # This file
```

---

## ðŸ“¦ Installation

```bash
git clone https://github.com/Buchhromain/MLP-From-Scratch.git
cd MLP-From-Scratch
npm install
```

> âš ï¸ If your project is written in JS (Node.js), adapt as needed (or use `python` if built in another language).

---

## ðŸš€ Run Training (Example)

```bash
node mlp.js --mode train --dataset mnist-in-csv/mnist_train.csv
```

---

## ðŸ“Š Expected Outputs

- âœ… Trained model saved to disk
- âœ… Confusion matrix displayed
- âœ… Loss curve generated (`loss_plot.html`)
- âœ… Evaluation on test set
- âœ… Model saved as `model.json`

---

## ðŸ“ˆ Example Visualization

*(To be added later: image of loss curve or confusion matrix screenshot)*

---

## ðŸ“„ Datasets

- [MNIST CSV](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv)
- [Boston Housing](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)

---

## ðŸ§  Framework-Free Design

This project is entirely handcrafted to provide a **deep understanding of deep learning** â€” from forward propagation to backpropagation.

---

## ðŸ“Œ To Do

- [ ] REST API Interface (e.g. Express or FastAPI)
- [ ] GPU support using WebGL or CUDA (optional)
- [ ] Simple user interface

---

## ðŸ™‹â€â™‚ï¸ Author

**Romain "Buchhromain"**  
> A passionate learner â€” this project demonstrates my knowledge and hands-on capability in designing neural networks from scratch.
